{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98397301-eb5e-4fc9-a022-246b4b4c408a",
   "metadata": {},
   "source": [
    "# Building a Transformer Encoder for Text Classification\n",
    "## 1 - Introduction\n",
    "In this notebook, you'll build a **Transformer Encoder** from scratch and apply it to sentiment analysis—classifying movie reviews as positive or negative. \n",
    "\n",
    "### What is a Transformer Encoder?\n",
    "A Transformer encoder is a neural network architecture that transforms input text into rich numerical representations. It reads an entire sequence of words at once and produces a contextualized representation for each word—meaning each word's representation contains information about how it relates to all other words in the sequence. Think of it as a sophisticated reading comprehension system that understands not just individual words, but their meanings in context.\n",
    "\n",
    "This hands-on implementation will deepen your understanding of how encoder models work and why they've become the backbone of models like BERT, RoBERTa, and other state-of-the-art NLP systems.\n",
    "\n",
    "### What You'll Build\n",
    "Starting from the attention mechanisms you've already mastered, you'll construct:\n",
    "1. A complete encoder block with multi-head attention and feed-forward layers\n",
    "2. A stack of encoder layers that progressively refine text representations\n",
    "3. A classification head that uses these representations for sentiment analysis\n",
    "4. A training pipeline that achieves strong performance on real movie reviews\n",
    "\n",
    "By the end of this notebook, you'll have implemented the same encoder architecture that, when pre-trained on massive text corpora, becomes BERT—one of the most important breakthroughs in modern NLP.\n",
    "\n",
    "### 1.1 Importing Necessary Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dcbd1d-bcad-4224-b9b7-f5115e0c2e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import random\n",
    "\n",
    "import helper_utils\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e98820-c295-4f88-94d9-72de85e60e41",
   "metadata": {},
   "source": [
    "## 2 - Building the Encoder Block\n",
    "### 2.1 Understanding the Encoder Architecture\n",
    "\n",
    "The encoder block is the fundamental building block of the Transformer encoder—a sophisticated text processing unit that refines word representations through attention and feed-forward mechanisms.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/encoder-block.svg\" alt=\"Encoder Block Architecture\" width=\"80%\">\n",
    "</div>\n",
    "\n",
    "#### Key Components\n",
    "\n",
    "**Multi-Head Attention**: Allows every word to gather information from all other words, with multiple parallel attention heads learning different types of relationships.\n",
    "\n",
    "**Feed-Forward Network**: Applies the same neural network independently to each position, adding non-linearity through two linear layers with ReLU activation.\n",
    "\n",
    "**Residual Connections** (green dashed lines): Create a direct path from input to output by adding the original input to each sub-layer's output. This preserves information and enables gradient flow through deep networks. Mathematically: `Output = Input + SubLayer(Input)`\n",
    "\n",
    "**Layer Normalization**: Stabilizes training by normalizing activations before each sub-layer.\n",
    "\n",
    "### 2.2 Implementing the Encoder Block\n",
    "\n",
    "Let's implement the encoder block, bringing together all these components into a modular unit that can be stacked to create increasingly sophisticated representations. Each additional layer captures more abstract patterns and longer-range dependencies.\n",
    "\n",
    "**Note:** This is a simplified version of an Encoder Layer. PyTorch's implementation includes additional parameters for more flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d94b9b-0290-410a-b1a1-3bfa2540d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model=4, nhead=1, ffn_mult=4):\n",
    "        super().__init__()\n",
    "        # Layer normalization before attention\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        # Multi-head self-attention\n",
    "        self.mha = nn.MultiheadAttention(d_model, nhead, batch_first=True)\n",
    "        # Layer normalization before feed-forward\n",
    "        self.ln2 = nn.LayerNorm(d_model)        \n",
    "        # Feed-forward network with expansion\n",
    "        hidden = ffn_mult * d_model\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, d_model)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):        \n",
    "        # First sub-layer: Multi-head attention with residual connection\n",
    "        x_norm = self.ln1(x)\n",
    "        attn_out, _ = self.mha(x_norm, x_norm, x_norm)\n",
    "        x = x + attn_out  # Residual connection\n",
    "        \n",
    "        # Second sub-layer: Feed-forward with residual connection\n",
    "        ffn_in = self.ln2(x)\n",
    "        ffn_out = self.ffn(ffn_in)\n",
    "        x = x + ffn_out  # Residual connection\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4407af-e9b7-4be4-a755-189a16de2810",
   "metadata": {},
   "source": [
    "### 2.3 Testing the Encoder Block\n",
    "Let's create a simple example to understand how the encoder block transforms input data. You'll create a small input tensor and pass it through the encoder to see the output shape and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01e41f7-f3ae-4bdd-a984-fd9a94040277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple encoder block with small dimensions for demonstration\n",
    "encoder_demo = EncoderBlock(d_model=4, nhead=1, ffn_mult=4)\n",
    "\n",
    "# Create a sample input: (batch_size=2, sequence_length=3, d_model=4)\n",
    "sample_input = torch.randn(2, 3, 4)\n",
    "\n",
    "print(\"Input shape:\", sample_input.shape)\n",
    "print(\"Input tensor:\\n\", sample_input)\n",
    "\n",
    "# Pass through encoder block\n",
    "output = encoder_demo(sample_input)\n",
    "\n",
    "print(\"\\nOutput shape:\", output.shape)\n",
    "print(\"Output tensor:\\n\", output)\n",
    "\n",
    "# Notice that the shape remains the same\n",
    "print(\"\\nShape preserved: Input shape == Output shape:\", sample_input.shape == output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff39a08a",
   "metadata": {},
   "source": [
    "### 2.4 Positional Encoding\n",
    "\n",
    "#### Sinusoidal Positional Encodings\n",
    "\n",
    "In the previous lab, you used learned positional embeddings. In this lab, you'll use a different approach: **sinusoidal positional encodings**—the method introduced in the original Transformer paper (\"Attention is All You Need\").\n",
    "\n",
    "Positional encoding adds a unique \"position signature\" to each token's embedding, like GPS coordinates for words. This tells the model not just *what* each word is, but *where* it appears in the sequence.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"images/positional-encoding.svg\" alt=\"Positional Encoding in Encoder\" width=\"70%\">\n",
    "</div>\n",
    "\n",
    "The encoder uses **sinusoidal functions** (sine and cosine waves at different frequencies) to create these position signatures:\n",
    "- Each position gets a unique pattern—like a mathematical fingerprint\n",
    "- The model can learn relative distances between any two positions\n",
    "- Patterns naturally extend to sequences longer than training data\n",
    "\n",
    "This enables the model to understand both **absolute position** (\"3rd word\") and **relative position** (\"4 positions apart\"), crucial for understanding grammar and long-range dependencies.\n",
    "\n",
    "The key advantages of the sinusoidal approach are:\n",
    "- Zero learnable parameters\n",
    "- Can generalize to sequences longer than those seen during training\n",
    "- Mathematical properties that help the model learn relative positions\n",
    "- Has become the standard in production transformer implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1b3dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Adds positional information to token embeddings using sinusoidal patterns.\n",
    "    \n",
    "    Since transformers don't have inherent notion of sequence order (unlike RNNs),\n",
    "    we add positional encodings to give the model information about where each\n",
    "    token appears in the sequence.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_len, d_model):\n",
    "        \"\"\"\n",
    "        Initialize positional encoding matrix.\n",
    "        \n",
    "        Args:\n",
    "            max_len (int): Maximum sequence length the model will handle\n",
    "                          (e.g., 100 for sentences up to 100 tokens)\n",
    "            d_model (int): Dimension of the model's embeddings \n",
    "                          (e.g., 256 or 512 - must match embedding size)\n",
    "        \n",
    "        Creates a fixed sinusoidal pattern matrix of shape [max_len, d_model]\n",
    "        where each row represents the positional encoding for that position.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        \n",
    "        # Create div_term for the sinusoidal pattern\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
    "                           -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        \n",
    "        # Apply sin to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Apply cos to odd indices  \n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Register as buffer (not trained, but saved with model)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Return positional encodings for the input sequence length.\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): Token embeddings of shape [batch_size, seq_len, d_model]\n",
    "                       where seq_len <= max_len from initialization\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Positional encodings of shape [batch_size, seq_len, d_model]\n",
    "                   (same shape as input, ready to be added to embeddings)\n",
    "        \n",
    "        Example:\n",
    "            If x represents embeddings for \"I love cats\" (3 tokens):\n",
    "            - Input x shape: [batch_size, 3, 256]\n",
    "            - Output shape: [batch_size, 3, 256]\n",
    "            - Returns positions 0, 1, 2 encoded as 256-dim vectors\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        # Return ONLY the positional encodings (not added to x)\n",
    "        # The addition happens in the model's forward method\n",
    "        return self.pe[:, :seq_len, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9e799e",
   "metadata": {},
   "source": [
    "Let's visualize how positional encoding works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cf1811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create positional encoding and visualize\n",
    "d_model = 128\n",
    "max_len = 100\n",
    "pos_encoder = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "# Create dummy embeddings for a batch of sequences\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "dummy_embeddings = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "print(f\"Input embeddings shape: {dummy_embeddings.shape}\")\n",
    "print(f\"Input embeddings mean: {dummy_embeddings.mean():.4f}\")\n",
    "\n",
    "# Apply positional encoding\n",
    "output = pos_encoder(dummy_embeddings)\n",
    "print(f\"\\nOutput shape (unchanged): {output.shape}\")\n",
    "print(f\"Output mean (slightly different): {output.mean():.4f}\")\n",
    "\n",
    "# Visualize the positional encoding pattern for first 50 positions and dimensions\n",
    "import matplotlib.pyplot as plt\n",
    "pe_matrix = pos_encoder.pe[0, :50, :64].numpy()\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(pe_matrix, cmap='RdBu', aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Embedding Dimension')\n",
    "plt.ylabel('Position')\n",
    "plt.title('Positional Encoding Pattern (sin/cos waves)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb34715",
   "metadata": {},
   "source": [
    "### Understanding the Positional Encoding Visualization\n",
    "\n",
    "This heatmap shows the positional encoding values added to embeddings:\n",
    "\n",
    "- **X-axis**: Embedding dimensions (0-63)\n",
    "- **Y-axis**: Sequence positions (0-49)  \n",
    "- **Colors**: Blue = positive values, Red = negative values, White = near zero\n",
    "\n",
    "**Key Patterns**:\n",
    "- **Left side**: Fast alternating patterns (high frequency) - capture local/nearby position information\n",
    "- **Right side**: Smooth gradients (low frequency) - capture global/long-range position information\n",
    "- **Each row**: Has a unique pattern - this is how position 5 is distinguished from position 45\n",
    "\n",
    "The varying frequencies (fast to slow) allow the model to understand both local word relationships and long-distance dependencies in the sequence. Each position gets a unique \"fingerprint\" that the model learns to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9288879",
   "metadata": {},
   "source": [
    "### 2.5 Understanding Padding Masks in Transformers\n",
    "\n",
    "#### What is Padding and Why Do We Need It?\n",
    "\n",
    "In real-world NLP applications, sentences have varying lengths. To process them efficiently in batches, we need to make all sequences the same length by adding **padding tokens**. However, these padding tokens shouldn't contribute to our model's understanding of the actual text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdf2e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq, pad_idx=0):\n",
    "    \"\"\"\n",
    "    Creates a boolean mask for padding tokens.\n",
    "    \n",
    "    Args:\n",
    "        seq: Input sequence tensor [batch_size, seq_len]\n",
    "        pad_idx: Index used for padding (typically 0)\n",
    "    \n",
    "    Returns:\n",
    "        Boolean tensor where True = padding, False = real token\n",
    "    \"\"\"\n",
    "    return seq == pad_idx\n",
    "\n",
    "# Example usage\n",
    "import torch\n",
    "\n",
    "# Sample batch with padding\n",
    "batch = torch.tensor([\n",
    "    [2, 15, 89, 234, 3, 0, 0, 0],   # 5 real tokens, 3 padding\n",
    "    [2, 45, 67, 89, 123, 234, 3, 0], # 7 real tokens, 1 padding\n",
    "    [2, 56, 3, 0, 0, 0, 0, 0],       # 3 real tokens, 5 padding\n",
    "])\n",
    "\n",
    "# Create padding mask\n",
    "padding_mask = create_padding_mask(batch, pad_idx=0)\n",
    "print(\"Input batch shape:\", batch.shape)\n",
    "print(\"\\nPadding mask:\")\n",
    "print(padding_mask)\n",
    "print(\"\\nTrue = padding position, False = real token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d65130b-968f-47cb-970c-0d07a5a39067",
   "metadata": {},
   "source": [
    "## 3 - Data Preparation\n",
    "\n",
    "### 3.1 Loading the IMDB Dataset\n",
    "\n",
    "The IMDB movie review dataset is a standard benchmark for sentiment analysis. It contains 50,000 movie reviews split evenly into training and test sets, with each set containing 25,000 positive and 25,000 negative reviews.\n",
    "\n",
    "Let's download and prepare the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae24f20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset with default settings (2000 train, 500 test samples)\n",
    "train_reviews, train_labels, test_reviews, test_labels = helper_utils.get_imdb_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e268c884-1ee6-46ac-adaa-e2eeabde5383",
   "metadata": {},
   "source": [
    "### 3.4 Exploring the Data\n",
    "\n",
    "Let's examine the loaded data to understand what you're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e629a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.print_data_statistics(train_reviews, train_labels, test_reviews, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34d795b-47ee-4ee0-9811-ca3a3b305886",
   "metadata": {},
   "source": [
    "## 4 - Text Processing and Dataset Creation\n",
    "\n",
    "### 4.1 Building a Tokenizer\n",
    "\n",
    "Before you can feed text into the neural network, you need to convert words into numbers. You'll create a tokenizer that builds a vocabulary from the training data and converts text into sequences of token indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e42cf6d-e6ce-497f-8884-781e954d8afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBTokenizer:\n",
    "    def __init__(self, vocab_size=10000):\n",
    "        self.vocab_size = vocab_size\n",
    "        # Special tokens\n",
    "        self.word_to_idx = {'<pad>': 0, '<unk>': 1, '<sos>': 2, '<eos>': 3}\n",
    "        self.idx_to_word = {0: '<pad>', 1: '<unk>', 2: '<sos>', 3: '<eos>'}\n",
    "        self.word_freq = Counter()\n",
    "    \n",
    "    def build_vocab(self, texts, min_freq=2):\n",
    "        \"\"\"Build vocabulary from texts\"\"\"\n",
    "        print(\"Building vocabulary...\")\n",
    "        \n",
    "        # Count word frequencies\n",
    "        for text in texts:\n",
    "            words = self.tokenize(text)\n",
    "            self.word_freq.update(words)\n",
    "        \n",
    "        # Get most common words (reserve 4 spots for special tokens)\n",
    "        most_common = self.word_freq.most_common(self.vocab_size - 4)\n",
    "        \n",
    "        # Add words to vocabulary if they meet minimum frequency\n",
    "        idx = 4  # Start after special tokens\n",
    "        for word, freq in most_common:\n",
    "            if freq >= min_freq:\n",
    "                self.word_to_idx[word] = idx\n",
    "                self.idx_to_word[idx] = word\n",
    "                idx += 1\n",
    "        \n",
    "        print(f\"Vocabulary size: {len(self.word_to_idx)}\")\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Simple tokenization - split text into words\"\"\"\n",
    "        text = text.lower()\n",
    "        # Remove HTML tags\n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "        # Keep only letters and spaces (remove all punctuation)\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)\n",
    "        words = text.split()\n",
    "        return words\n",
    "        \n",
    "    def encode(self, text, max_len=256):\n",
    "        \"\"\"Convert text to indices\"\"\"\n",
    "        words = self.tokenize(text)[:max_len-2]  # Leave space for SOS/EOS tokens\n",
    "        \n",
    "        # Start with SOS token\n",
    "        indices = [2]\n",
    "        \n",
    "        # Convert words to indices\n",
    "        for word in words:\n",
    "            if word in self.word_to_idx:\n",
    "                indices.append(self.word_to_idx[word])\n",
    "            else:\n",
    "                indices.append(1)  # Unknown word -> UNK token\n",
    "        \n",
    "        # Add EOS token\n",
    "        indices.append(3)\n",
    "        \n",
    "        # Pad sequence to max_len\n",
    "        while len(indices) < max_len:\n",
    "            indices.append(0)  # Pad token\n",
    "        \n",
    "        return indices[:max_len]  # Ensure exactly max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed52bda-520c-4adc-a6f5-36d3df4f4f3b",
   "metadata": {},
   "source": [
    "### 4.2 Creating and Testing the Tokenizer\n",
    "\n",
    "Let's create the tokenizer and test it with sample text to see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0de08d-d2f8-43cb-91f2-392aa8dc5f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = IMDBTokenizer(vocab_size=5000)\n",
    "\n",
    "# Build vocabulary from training reviews\n",
    "tokenizer.build_vocab(train_reviews, min_freq=2)\n",
    "\n",
    "# Test tokenizer with a sample sentence\n",
    "sample_text = \"This movie was absolutely fantastic! I loved every minute of it.\"\n",
    "print(\"Original text:\", sample_text)\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(sample_text)\n",
    "print(\"\\nTokenized:\", tokens)\n",
    "\n",
    "# Encode to indices\n",
    "encoded = tokenizer.encode(sample_text, max_len=20)\n",
    "print(\"\\nEncoded (max_len=20):\", encoded)\n",
    "\n",
    "# Decode back to words to verify\n",
    "decoded_words = [tokenizer.idx_to_word.get(idx, '<unk>') for idx in encoded]\n",
    "print(\"\\nDecoded words:\", decoded_words)\n",
    "\n",
    "# Show some vocabulary statistics\n",
    "print(f\"\\nVocabulary Statistics:\")\n",
    "print(f\"Total unique words in vocab: {len(tokenizer.word_to_idx)}\")\n",
    "print(f\"Most common words: {tokenizer.word_freq.most_common(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b5b7af-0862-4a6c-967d-1f093fe0494c",
   "metadata": {},
   "source": [
    "### 4.3 Creating PyTorch Dataset\n",
    "\n",
    "Now you'll create a PyTorch Dataset class that will handle the data loading and preprocessing for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d6d0d3-5d15-4bf2-b19d-c87e93d3c9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, reviews, labels, tokenizer, max_len=256):\n",
    "        \"\"\"\n",
    "        Initialize IMDB dataset\n",
    "        Args:\n",
    "            reviews: List of review texts\n",
    "            labels: List of labels (0 or 1)\n",
    "            tokenizer: Tokenizer object for encoding text\n",
    "            max_len: Maximum sequence length\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        \n",
    "        print(f\"Processing {len(reviews)} reviews...\")\n",
    "        \n",
    "        # Encode all reviews\n",
    "        for review, label in zip(reviews, labels):\n",
    "            encoded = tokenizer.encode(review, max_len)\n",
    "            self.data.append(encoded)\n",
    "            self.labels.append(label)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        self.data = torch.LongTensor(self.data)\n",
    "        self.labels = torch.LongTensor(self.labels)\n",
    "        \n",
    "        print(f\"Dataset created with shape: {self.data.shape}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the size of the dataset\"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get a single item from the dataset\"\"\"\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644f7d85-6937-4ebc-a1f0-c0db5b0053a5",
   "metadata": {},
   "source": [
    "### 4.4 Creating Train and Test Datasets\n",
    "\n",
    "Let's create the dataset objects and data loaders for training:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7243259f-d401-4c65-9110-2761acff0864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "max_seq_length = 256  # Maximum sequence length\n",
    "\n",
    "print(\"Creating training dataset...\")\n",
    "train_dataset = IMDBDataset(train_reviews, train_labels, tokenizer, max_len=max_seq_length)\n",
    "\n",
    "print(\"\\nCreating test dataset...\")\n",
    "test_dataset = IMDBDataset(test_reviews, test_labels, tokenizer, max_len=max_seq_length)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"\\nData loaders created:\")\n",
    "print(f\"  Training batches: {len(train_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Examine a single batch\n",
    "sample_batch, sample_labels = next(iter(train_loader))\n",
    "print(f\"\\nSample batch shape: {sample_batch.shape}\")\n",
    "print(f\"Sample labels shape: {sample_labels.shape}\")\n",
    "print(f\"First sequence in batch (first 20 tokens): {sample_batch[0, :20].tolist()}\")\n",
    "print(f\"Label for first sequence: {sample_labels[0].item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94f64b8-da8f-4f7b-9c33-7fd0cd2b5673",
   "metadata": {},
   "source": [
    "## 5 - Building the Sentiment Classification Model with Custom Encoder\n",
    "\n",
    "### 5.1 Complete Model Architecture\n",
    "\n",
    "Now you'll combine the encoder blocks into a complete model for sentiment classification. The model will:\n",
    "1. Convert token indices to embeddings\n",
    "2. Add positional embeddings to preserve sequence order\n",
    "3. Pass through multiple encoder blocks\n",
    "4. Pool the sequence representation\n",
    "5. Classify into positive or negative sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c594cb-4493-470d-9407-b8b4484288ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBSentimentModelWithCustomEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, num_layers=2, max_len=512, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Sentiment classifier using stacked encoder blocks with positional encoding\n",
    "        \n",
    "        Args:\n",
    "            vocab_size: Size of vocabulary\n",
    "            d_model: Dimension of embeddings\n",
    "            num_layers: Number of encoder blocks to stack (default: 2)\n",
    "            max_len: Maximum sequence length for positional encoding (default: 512)\n",
    "            dropout: Dropout probability for regularization (default: 0.1)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store d_model for debugging\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Convert word indices to vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        \n",
    "        # IMPORTANT: Make sure PositionalEncoding uses the SAME d_model\n",
    "        self.positional_encoding = PositionalEncoding(max_len, d_model)  # d_model must match!\n",
    "        \n",
    "        # Dropout after embeddings + positional encoding (standard practice)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Stack multiple encoder blocks\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderBlock(d_model=d_model, nhead=8, ffn_mult=4) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Simple classifier\n",
    "        self.classifier = nn.Linear(d_model, 2)  # 2 classes: negative and positive\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of token indices (batch_size, seq_len)\n",
    "        Returns:\n",
    "            Classification logits (batch_size, 2)\n",
    "        \"\"\"\n",
    "        # Step 1: Convert tokens to embeddings\n",
    "        x = self.embedding(x)  # Shape: (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Step 2: Add positional encoding to embeddings\n",
    "        pos_encoding = self.positional_encoding(x)\n",
    "        \n",
    "        x = x + pos_encoding\n",
    "        \n",
    "        # Step 3: Apply dropout for regularization\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Step 4: Pass through each encoder layer sequentially\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(x)  # Shape: (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Step 5: Simple pooling - average all tokens\n",
    "        x = x.mean(dim=1)  # Shape: (batch_size, d_model)\n",
    "        \n",
    "        # Step 6: Classify\n",
    "        output = self.classifier(x)  # Shape: (batch_size, 2)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92f5814-8fa9-42ae-86c4-13f5333d5d32",
   "metadata": {},
   "source": [
    "### 5.2 Creating and Testing the Model\n",
    "\n",
    "Let's create an instance of the model and test it with a sample batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec11e6e-53c0-4488-9cad-e908d799c0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vocabulary size\n",
    "vocab_size = len(tokenizer.word_to_idx)\n",
    "\n",
    "# Create the model\n",
    "model = IMDBSentimentModelWithCustomEncoder(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=128,\n",
    "    num_layers = 2\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model created: IMDBSentimentModel\")\n",
    "print(f\"Total learnable parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5996b5b6-4bd3-4fe8-bd29-65f219f6e00c",
   "metadata": {},
   "source": [
    "### 5.3 Understanding the Forward Pass\n",
    "\n",
    "Let's trace through the model step by step with a small example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0cda0d-666c-4009-879b-88fe5a20c527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple input: batch of 2 sequences, each with 5 tokens\n",
    "tiny_input = torch.tensor([\n",
    "    [2, 45, 23, 67, 3],  # First review: <sos>, word45, word23, word67, <eos>\n",
    "    [2, 12, 89, 34, 3]   # Second review: <sos>, word12, word89, word34, <eos>\n",
    "])\n",
    "print(\"=== Step-by-step Forward Pass ===\\n\")\n",
    "print(\"Input (token indices):\")\n",
    "print(tiny_input)\n",
    "print(\"Shape:\", tiny_input.shape, \"\\n\")\n",
    "\n",
    "# Step through the model manually\n",
    "with torch.no_grad():\n",
    "    # Step 1: Embedding\n",
    "    embedded = model.embedding(tiny_input)\n",
    "    print(\"Step 1 - After embedding:\")\n",
    "    print(\"  Shape:\", embedded.shape)\n",
    "    print(\"  Each token is now a vector of size\", embedded.shape[-1])\n",
    "    \n",
    "    # Step 2: Pass through encoder layers\n",
    "    x = embedded\n",
    "    print(f\"\\nStep 2 - Passing through {len(model.encoder_layers)} encoder layer(s):\")\n",
    "    for i, encoder_layer in enumerate(model.encoder_layers):\n",
    "        x = encoder_layer(x)\n",
    "        print(f\"  After encoder layer {i+1}:\")\n",
    "        print(f\"    Shape: {x.shape}\")\n",
    "        if i == 0:\n",
    "            print(\"    Note: shape is unchanged, but representations are refined\")\n",
    "    \n",
    "    encoded = x\n",
    "    print(f\"\\nAfter all encoder layers:\")\n",
    "    print(f\"  Final encoded shape: {encoded.shape}\")\n",
    "    print(\"  Each token now has a contextualized representation\")\n",
    "    \n",
    "    # Step 3: Pooling\n",
    "    pooled = encoded.mean(dim=1)\n",
    "    print(\"\\nStep 3 - After averaging across sequence:\")\n",
    "    print(\"  Shape:\", pooled.shape)\n",
    "    print(\"  Now we have one vector per review\")\n",
    "    \n",
    "    # Step 4: Classification\n",
    "    output = model.classifier(pooled)\n",
    "    print(\"\\nStep 4 - Final classification scores:\")\n",
    "    print(\"  Shape:\", output.shape)\n",
    "    print(\"  Raw scores (logits):\", output)\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    probs = torch.softmax(output, dim=1)\n",
    "    print(\"\\n  Probabilities [negative, positive]:\")\n",
    "    for i, p in enumerate(probs):\n",
    "        print(f\"    Review {i}: [{p[0]:.3f}, {p[1]:.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e616ef42-9c5b-43ee-87ef-64253de6c862",
   "metadata": {},
   "source": [
    "### 5.4 Testing with Real Data\n",
    "Let's test the model with actual review data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f24ffcf-500a-4bdb-8fa8-24237cbca4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch from our data loader\n",
    "sample_input, sample_labels = next(iter(train_loader))\n",
    "\n",
    "print(\"Testing with real data:\")\n",
    "print(f\"Batch shape: {sample_input.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    output = model(sample_input)\n",
    "\n",
    "# Get predictions\n",
    "predictions = torch.argmax(output, dim=1)\n",
    "accuracy = (predictions == sample_labels).float().mean()\n",
    "\n",
    "print(f\"\\nResults on this batch (untrained model):\")\n",
    "print(f\"  Accuracy: {accuracy:.1%}\")\n",
    "print(f\"\\nFirst 5 predictions vs actual:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i in range(5):\n",
    "    pred_label = \"Positive\" if predictions[i] == 1 else \"Negative\"\n",
    "    true_label = \"Positive\" if sample_labels[i] == 1 else \"Negative\"\n",
    "    correct = \"✓\" if predictions[i] == sample_labels[i] else \"✗\"\n",
    "    \n",
    "    # Decode the tokens back to words (first 30 tokens for brevity)\n",
    "    tokens = sample_input[i][:30].tolist()\n",
    "    # Remove padding tokens (0s) from the end\n",
    "    tokens = [t for t in tokens if t != 0]\n",
    "    # Convert token IDs back to words\n",
    "    words = [tokenizer.idx_to_word.get(token_id, '<unk>') for token_id in tokens]\n",
    "    # Join into sentence\n",
    "    sentence_preview = ' '.join(words[:15]) + '...'  # Show first 15 words\n",
    "    \n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(f\"  Text: {sentence_preview}\")\n",
    "    print(f\"  Predicted: {pred_label}, Actual: {true_label} {correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d898b2a-6d37-49b0-9334-1f11cad456a5",
   "metadata": {},
   "source": [
    "### 5.5 Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d67604",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.print_summary(model, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9544f3df-77e5-4ddc-831e-10f1d061bb72",
   "metadata": {},
   "source": [
    "## 6 - Training the Model\n",
    "\n",
    "### 6.1 Setting Up Training Components\n",
    "\n",
    "To train the model, you need to set up the loss function and optimizer. For binary classification, you'll use Cross Entropy Loss, and for optimization, you'll use Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31123a6-71a6-4761-b655-adae55d2cffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function for classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer - Adam with learning rate 0.001\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"Training setup:\")\n",
    "print(f\"  Loss function: CrossEntropyLoss\")\n",
    "print(f\"  Optimizer: Adam\")\n",
    "print(f\"  Learning rate: {learning_rate}\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Training samples: {len(train_dataset)}\")\n",
    "print(f\"  Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d603811-1b3a-47e0-8665-68b107bd0da5",
   "metadata": {},
   "source": [
    "### 6.2 Evaluating Before Training\n",
    "\n",
    "Let's first check how the untrained model performs on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38424ec-3d3d-47b2-9653-39208c8592ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Evaluate untrained model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "print(\"Evaluating untrained model...\")\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "initial_accuracy = 100 * correct / total\n",
    "print(f\"Initial Test Accuracy (before training): {initial_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1291d78-1171-4e98-86c6-5beb24e72403",
   "metadata": {},
   "source": [
    "### 6.3 Training The Model!\n",
    "\n",
    "Now run the next cell to train your model with a custom Encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815a132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = helper_utils.train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    num_epochs=5,\n",
    "    device=device  # Optional, will auto-detect if not provided\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66baf59b-797e-4234-840b-4293cb1543bb",
   "metadata": {},
   "source": [
    "### 6.5 Visualizing Training Progress\n",
    "\n",
    "Let's plot the training history to see how the model learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9970b9-8fa7-4486-86be-4d96b4eee20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeda2e4-e2d9-4812-8c2c-bf1a2049a5e6",
   "metadata": {},
   "source": [
    "## 7 - Using PyTorch's Built-in Transformer Encoder\n",
    "### 7.1 Creating a General-Purpose Encoder with PyTorch\n",
    "Now that you understand the encoder architecture from building it yourself, let's leverage PyTorch's optimized implementation for a production-ready model. PyTorch provides `nn.TransformerEncoder` which handles all the complexity you've just implemented, but with additional optimizations for speed and memory efficiency.\n",
    "\n",
    "#### A Versatile Encoder Design\n",
    "The encoder we're building is designed to be versatile - it can be used standalone for tasks like classification, or as part of a larger encoder-decoder architecture for tasks like translation. This flexibility is achieved through a key design decision: **the encoder always returns both the encoded representation and the padding mask**.\n",
    "\n",
    "**What is \"Memory\" in Transformer Context?**\n",
    "In transformer terminology, \"memory\" refers to the encoded representation of the input sequence. It's called \"memory\" because:\n",
    "- It contains all the learned contextual information from the input\n",
    "- In encoder-decoder models, the decoder \"remembers\" and attends to this information\n",
    "- Each position in the memory contains a rich, contextualized representation of the corresponding input token\n",
    "\n",
    "#### The Encoder's Dual Output\n",
    "Our encoder returns two crucial components:\n",
    "1. **Memory** (`[batch_size, seq_len, d_model]`): The contextualized representations of all input tokens\n",
    "2. **Padding Mask** (`[batch_size, seq_len]`): A boolean mask indicating which positions are padding\n",
    "\n",
    "This dual output makes the encoder incredibly flexible:\n",
    "- **For classification**: Use the memory with a pooling strategy and classification head\n",
    "- **For translation**: Pass both memory and mask to a decoder\n",
    "- **For sequence tagging**: Use the memory directly for token-level predictions\n",
    "\n",
    "#### From Text to Encoded Representations: The Pipeline\n",
    "Here's how our encoder processes text:\n",
    "```\n",
    "Input Text: \"This movie is great\"\n",
    "         ↓\n",
    "    [Tokenization]\n",
    "         ↓\n",
    "    Token IDs: [101, 2023, 3185, 2003, 2307, 0, 0]  (with padding)\n",
    "         ↓\n",
    "    [Token Embedding]\n",
    "         ↓\n",
    "    [Add Positional Encoding]\n",
    "         ↓\n",
    "    [Create Padding Mask]\n",
    "         ↓\n",
    "    [Transformer Encoder Stack]\n",
    "         ↓\n",
    "    Outputs:\n",
    "    - Memory: Contextualized representations for all positions\n",
    "    - Padding Mask: [True, True, True, True, True, False, False]\n",
    "```\n",
    "\n",
    "#### Key Design Features\n",
    "**Built-in Padding Support:**\n",
    "- The encoder automatically creates padding masks from input token IDs\n",
    "- Assumes padding token ID is 0 (standard convention)\n",
    "- Ensures padding tokens don't influence the attention mechanism\n",
    "\n",
    "**Architectural Parameters:**\n",
    "- `vocab_size`: Size of the token vocabulary\n",
    "- `d_model`: Dimension of token representations (256 by default)\n",
    "- `nhead`: Number of attention heads for multi-perspective learning\n",
    "- `num_layers`: Depth of the encoder stack\n",
    "- `dim_feedforward`: Hidden dimension in feedforward networks\n",
    "- `max_len`: Maximum sequence length supported\n",
    "- `dropout`: Regularization to prevent overfitting\n",
    "\n",
    "**Why This Design Matters:**\n",
    "- **Flexibility**: Same encoder works for multiple downstream tasks\n",
    "- **Efficiency**: Padding masks prevent unnecessary computation on padding tokens\n",
    "- **Modularity**: Can easily plug this encoder into different architectures\n",
    "- **Production-ready**: Uses PyTorch's optimized transformer implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f353460-46e5-4f52-be0c-0f8c25d2f7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    General-purpose Encoder that can be used standalone (classification) \n",
    "    or as part of an encoder-decoder model (translation).\n",
    "    \n",
    "    This encoder includes padding mask support, which is especially important\n",
    "    when used with a decoder for tasks like translation.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model=256, nhead=8, num_layers=3, \n",
    "                 dim_feedforward=512, max_len=100, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.nhead = nhead\n",
    "        self.num_layers = num_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.max_len = max_len\n",
    "        self.dropout_value = dropout\n",
    "        \n",
    "        # Token embedding\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_enc = PositionalEncoding(max_len, d_model)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "         \n",
    "        # Transformer encoder layers\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=dim_feedforward, \n",
    "            dropout=dropout, \n",
    "            batch_first=True \n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Token indices [batch_size, seq_len]\n",
    "        Returns:\n",
    "            memory: Encoded representation [batch_size, seq_len, d_model]\n",
    "            padding_mask: Boolean mask for padding positions\n",
    "        \"\"\"\n",
    "        # Create padding mask\n",
    "        padding_mask = create_padding_mask(src, pad_idx=0)\n",
    "        \n",
    "        # Embed tokens and add positional encoding\n",
    "        src = self.token_emb(src) + self.pos_enc(src)\n",
    "        \n",
    "        # Apply dropout\n",
    "        src = self.dropout(src)\n",
    "        \n",
    "        # Pass through transformer encoder\n",
    "        memory = self.transformer_encoder(src, src_key_padding_mask=padding_mask)\n",
    "        \n",
    "        # Always return both memory and padding mask\n",
    "        return memory, padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbbbec0",
   "metadata": {},
   "source": [
    "### 7.2 Encoder Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943503b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize your encoder\n",
    "encoder = Encoder(\n",
    "    vocab_size=1000,\n",
    "    d_model=128,\n",
    "    nhead=8,\n",
    "    num_layers=2,\n",
    "    dim_feedforward=512,\n",
    "    max_len=100,\n",
    "    dropout=0.1\n",
    ")\n",
    "encoder.eval()\n",
    "\n",
    "# Sample input\n",
    "input_batch = torch.tensor([\n",
    "    [2, 45, 23, 3, 0, 0],  # 4 tokens + 2 padding\n",
    "    [2, 12, 89, 34, 56, 3]  # 6 tokens, no padding\n",
    "])\n",
    "\n",
    "print(\"Input:\", input_batch)\n",
    "print()\n",
    "\n",
    "# Your encoder always returns both output and padding mask\n",
    "with torch.no_grad():\n",
    "    memory, padding_mask = encoder(input_batch)\n",
    "\n",
    "print(\"Encoder returns:\")\n",
    "print(f\"  memory shape: {memory.shape}\")\n",
    "print(f\"  padding_mask shape: {padding_mask.shape}\")\n",
    "print(f\"  padding_mask: {padding_mask}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3170a3-567a-44a7-99a4-30e8d1746691",
   "metadata": {},
   "source": [
    "### 7.3 Creating and Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7500d4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_model = Encoder(vocab_size=vocab_size, d_model=128, max_len = 256, num_layers = 2).to(device)\n",
    "\n",
    "helper_utils.print_summary(pytorch_model, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a0b531-7689-431c-8f70-93816b923e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample batch\n",
    "sample_input, sample_labels = next(iter(train_loader))\n",
    "sample_input = sample_input.to(device)\n",
    "\n",
    "output = pytorch_model(sample_input)\n",
    "print(f\"\\nTest forward pass:\")\n",
    "print(f\"  Input shape: {sample_input.shape}\")\n",
    "print(f\"  Output shape: {output[0].shape}\")\n",
    "print(\"  Model works! ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46abf8ce",
   "metadata": {},
   "source": [
    "### 7.4 The IMDB Classifier Using PyTorch Encoder\n",
    "\n",
    "Now that we have our general-purpose Encoder, let's build a sentiment classifier for the IMDB dataset by adding a classification head on top.\n",
    "\n",
    "#### Building a Task-Specific Model\n",
    "Our approach is straightforward:\n",
    "1. Use the Encoder to get contextualized representations (memory) of the input\n",
    "2. Pool the sequence representations into a single vector\n",
    "3. Add a linear classification head to predict sentiment\n",
    "\n",
    "This modular design shows the power of the encoder architecture - we can reuse the same encoder for different tasks by simply changing what we put on top of it.\n",
    "\n",
    "```\n",
    "Input Tokens → Encoder → Memory & Mask → Mean Pooling → Linear Layer → Sentiment\n",
    "                  ↓                           ↓                ↓\n",
    "          Contextual Representations    Single Vector    Pos/Neg Score\n",
    "```\n",
    "\n",
    "For this classifier, we use mean pooling to aggregate the sequence (simple and effective for sentiment analysis) and a single linear layer for classification. The encoder does the heavy lifting of understanding context, while the classification head just needs to map the pooled representation to sentiment scores.\n",
    "\n",
    "Let's implement this classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39481abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBClassifierWithPytorchEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple sentiment classifier using the Encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model=128, nhead=8, num_layers=2, \n",
    "                 dim_feedforward=512, max_len=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Use the Encoder\n",
    "        self.encoder = Encoder(\n",
    "            vocab_size=vocab_size,\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_layers=num_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            max_len=max_len,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Simple classifier (just a linear layer)\n",
    "        self.classifier = nn.Linear(d_model, 2)  # 2 classes: positive/negative\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input token indices [batch_size, seq_len]\n",
    "        Returns:\n",
    "            logits: Classification scores [batch_size, 2]\n",
    "        \"\"\"\n",
    "        # Get encoder outputs\n",
    "        memory, padding_mask = self.encoder(x)\n",
    "        \n",
    "        # Simple pooling - just average all positions\n",
    "        # (padding tokens have small values so impact is minimal)\n",
    "        pooled = memory.mean(dim=1)  # [batch_size, d_model]\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.classifier(pooled)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228a625e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the simple classifier\n",
    "pytorch_model = IMDBClassifierWithPytorchEncoder(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=128,\n",
    "    num_layers=2,\n",
    "    max_len=256\n",
    ").to(device)\n",
    "\n",
    "helper_utils.print_summary(pytorch_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afad72cc-3f01-4172-be5f-515ae740a2d6",
   "metadata": {},
   "source": [
    "### 7.3 Training the PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e723cb-0b7b-48ef-8058-3dad136a8982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function for classification\n",
    "pytorch_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer - Adam with learning rate 0.001\n",
    "learning_rate = 0.001\n",
    "pytorch_optimizer = optim.Adam(pytorch_model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"Training setup:\")\n",
    "print(f\"  Loss function: CrossEntropyLoss\")\n",
    "print(f\"  Optimizer: Adam\")\n",
    "print(f\"  Learning rate: {learning_rate}\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Training samples: {len(train_dataset)}\")\n",
    "print(f\"  Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c157f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now train\n",
    "EPOCHS = 5\n",
    "pytorch_history = helper_utils.train_model(\n",
    "    pytorch_model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    pytorch_optimizer,\n",
    "    pytorch_criterion,\n",
    "    num_epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521d2d46",
   "metadata": {},
   "source": [
    "### 7.4 Comparing Both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4872eafb-91fa-4ea9-abd6-61cb0ef1b884",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.compare_models(history, pytorch_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9dd23f",
   "metadata": {},
   "source": [
    "### Model Performance Analysis with Best Epoch Selection\n",
    "\n",
    "After implementing early stopping (selecting the best epoch rather than the final one), you can better compare both models' true performance:\n",
    "\n",
    "#### The Impact of Early Stopping\n",
    "\n",
    "**Custom Model:**\n",
    "- Typically reaches best performance in later epochs (4-5)\n",
    "- Train-test gap: ~8-10% (mild overfitting)\n",
    "- Often continues improving throughout training\n",
    "\n",
    "**PyTorch Model:**\n",
    "- Usually peaks earlier (epochs 2-4)\n",
    "- Train-test gap: Often very small or even negative (excellent generalization)\n",
    "- Performance sometimes degrades after peaking\n",
    "\n",
    "#### Performance Comparison\n",
    "\n",
    "**The results are remarkably close** - across different runs, sometimes the custom model performs better, sometimes the PyTorch model does. The difference is typically within 1-2%, which is essentially statistical noise. This variability shows that:\n",
    "\n",
    "- Both architectures have similar capacity for this task\n",
    "- Random initialization and training dynamics affect final performance\n",
    "- The IMDB sentiment task isn't complex enough to strongly differentiate the models\n",
    "\n",
    "#### Why Different Stopping Points?\n",
    "\n",
    "**Custom Model**: Less regularization allows continued learning\n",
    "- Can extract more patterns from the data over time\n",
    "- Risk of eventual overfitting\n",
    "\n",
    "**PyTorch Model**: Heavy regularization creates an earlier peak\n",
    "- Multiple dropout layers prevent overlearning\n",
    "- Model finds its sweet spot faster but can degrade with too much training\n",
    "\n",
    "#### Model Confidence Analysis\n",
    "\n",
    "One consistent difference across runs:\n",
    "- **Custom model**: Makes confident predictions (70-90% certainty)\n",
    "- **PyTorch model**: More hesitant predictions (50-60% certainty)\n",
    "\n",
    "This confidence gap persists regardless of which model achieves better accuracy, suggesting it's an inherent characteristic of the architectures rather than a performance indicator.\n",
    "\n",
    "#### Key Insights\n",
    "\n",
    "1. **Both approaches work**: The similar performance validates both implementations\n",
    "2. **Early stopping matters more for regularized models**: PyTorch model benefits more from finding its optimal epoch\n",
    "3. **Confidence ≠ Accuracy**: The custom model is more decisive but not necessarily more correct\n",
    "\n",
    "#### The Bottom Line\n",
    "\n",
    "With proper early stopping, both models achieve comparable performance (typically 73-76% test accuracy). The \"winner\" varies by run, indicating that:\n",
    "- Architecture differences matter less than training dynamics for this task\n",
    "- Both models are essentially solving the problem equally well\n",
    "- The choice between them might depend on other factors (confidence requirements, training stability, etc.)\n",
    "\n",
    "This demonstrates an important lesson: for many real-world tasks, simple and complex architectures can achieve similar results, and the best choice depends on your specific requirements beyond just accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061938e3-75ae-4478-9e80-c5faa21e1cab",
   "metadata": {},
   "source": [
    "## 8 - Testing Models with Real Text\n",
    "### 8.1 Creating a Prediction Function\n",
    "Let's create a function to predict sentiment from any text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da1c0cc-607e-4ddc-8a3a-3b4dcf634b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, tokenizer, device):\n",
    "    \"\"\"\n",
    "    Predict sentiment for a given text\n",
    "    Args:\n",
    "        text: Input text string\n",
    "        model: Trained model\n",
    "        tokenizer: Tokenizer used during training\n",
    "        device: Device (cpu or cuda)\n",
    "    Returns:\n",
    "        Prediction and confidence\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize and encode the text\n",
    "    encoded = tokenizer.encode(text, max_len=256)\n",
    "    input_tensor = torch.LongTensor([encoded]).to(device)\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        probabilities = torch.softmax(output, dim=1)\n",
    "        prediction = torch.argmax(output, dim=1)\n",
    "    \n",
    "    # Get confidence\n",
    "    confidence = probabilities[0][prediction].item()\n",
    "    sentiment = \"Positive\" if prediction.item() == 1 else \"Negative\"\n",
    "    \n",
    "    return sentiment, confidence, probabilities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8640df-5766-4dae-b6d7-c0a7ce0d69dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reviews\n",
    "test_reviews = [\n",
    "    \"This movie was absolutely fantastic! I loved every minute of it.\",\n",
    "    \"Terrible film. Complete waste of time. I want my money back.\",\n",
    "    \"Not bad, but not great either. It was okay I guess.\",\n",
    "    \"One of the best films I've ever seen. Brilliant acting and amazing story!\",\n",
    "    \"Boring and predictable. I fell asleep halfway through.\"\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TESTING BOTH MODELS WITH SAMPLE REVIEWS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, review in enumerate(test_reviews, 1):\n",
    "    print(f\"\\nReview {i}: \\\"{review[:50]}...\\\"\" if len(review) > 50 else f\"\\nReview {i}: \\\"{review}\\\"\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # Test with encoder from scratch\n",
    "    sentiment, confidence, probs = predict_sentiment(review, model, tokenizer, device)\n",
    "    print(f\"Encoder from Scratch: {sentiment} (confidence: {confidence:.2%})\")\n",
    "    print(f\"  [Negative: {probs[0]:.3f}, Positive: {probs[1]:.3f}]\")\n",
    "    \n",
    "    # Test with PyTorch implemented encoder\n",
    "    sentiment_pt, confidence_pt, probs_pt = predict_sentiment(review, pytorch_model, tokenizer, device)\n",
    "    print(f\"PyTorch Implemented Encoder: {sentiment_pt} (confidence: {confidence_pt:.2%})\")\n",
    "    print(f\"  [Negative: {probs_pt[0]:.3f}, Positive: {probs_pt[1]:.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604f0490-ea65-4c57-a05d-7c6ac3db78d7",
   "metadata": {},
   "source": [
    "## 9 - Conclusion\n",
    "In this notebook, you successfully built a Transformer Encoder from scratch and applied it to sentiment analysis. You learned the fundamental components of the encoder architecture including multi-head attention, feed-forward networks, layer normalization, and residual connections.\n",
    "\n",
    "By comparing the encoder built from scratch with PyTorch's built-in TransformerEncoder, you discovered that simpler architectures can often perform better on small datasets. The custom encoder from scratch achieved competitive accuracy despite having fewer parameters and layers than the PyTorch implemented encoder model.\n",
    "\n",
    "The key takeaway is that understanding the architecture deeply by building it yourself gives you the intuition to make better design choices for your specific problem. You now have the foundation to experiment with more complex transformer architectures and apply them to various NLP tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}